{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from livenet.backend.core import Context\n",
    "import random\n",
    "import importlib\n",
    "import onnx\n",
    "import livenet\n",
    "device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor()\n",
    "#         ,transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#      ])\n",
    "# train=torchvision.datasets.CIFAR10(\"/home/spometun/datasets/research\", train=True,\n",
    "#                                   download=True, transform=transform)\n",
    "# test=torchvision.datasets.CIFAR10(\"/home/spometun/datasets/research\", train=False,\n",
    "#                                   download=True, transform=transform)\n",
    "#\n",
    "#\n",
    "# def get_whole_data(dataset):\n",
    "#     loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
    "#     data = next(iter(loader))\n",
    "#     data[0] = data[0].to(device)\n",
    "#     data[1] = data[1].to(device)\n",
    "#     return data\n",
    "#\n",
    "# test_whole_data = get_whole_data(test)\n",
    "# train_whole_data = get_whole_data(train)\n",
    "\n",
    "# test_x, test_y = test_whole_data\n",
    "# train_x, train_y = train_whole_data\n",
    "\n",
    "test_x, test_y = livenet.datasets.get_cifar10_test()\n",
    "train_x, train_y = livenet.datasets.get_cifar10_train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(0)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.context = Context(self)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3,padding=\"same\")\n",
    "        self.conv3 = nn.Conv2d(16, 16, 3,padding=\"same\")\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.fc = nn.Linear(8*7*7,10)\n",
    "        self._alpha = 0.0001\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # x = self.pool(x)\n",
    "        # x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def internal_loss(self):\n",
    "        loss = torch.tensor(0.)\n",
    "        for param in self.parameters():\n",
    "            if len(param.data.shape) > 1:\n",
    "                loss += self._alpha * torch.sum(torch.abs(param)) / param.data.numel()\n",
    "        return loss\n",
    "\n",
    "\n",
    "network = Net()\n",
    "torchsummary.summary(network, (3, 32, 32), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "batch_iterator = livenet.gen_utils.batch_iterator(train_x, train_y, batch_size)\n",
    "criterion = livenet.nets.criterion_classification_n\n",
    "optimizer = livenet.nets.create_optimizer(network)\n",
    "trainer = livenet.net_trainer.NetTrainer(network, batch_iterator, criterion, optimizer, epoch_size=len(train_x) // batch_size)\n",
    "trainer.adaptive_lr = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.step(5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_stats(network, data):\n",
    "    with torch.no_grad():\n",
    "        outputs = network(data[0])\n",
    "    labels = data[1]\n",
    "\n",
    "    total = len(labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    _loss = criterion(outputs, labels)\n",
    "    _reg_loss = net.reg_loss_func()\n",
    "    print(f\"Loss {_loss:.2f} {_reg_loss:.2f} Accuracy {100 * correct / total:.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func():\n",
    "    return livenet.gen_utils.batch_iterator(*test_whole_data, batch_size=256, only_one_epoch=True)\n",
    "\n",
    "def train_func():\n",
    "    return livenet.gen_utils.batch_iterator(*train_whole_data, batch_size=256, only_one_epoch=True)\n",
    "import time\n",
    "\n",
    "net._alpha = 0.0001\n",
    "\n",
    "def criterion(input, target):\n",
    "    return nn.functional.cross_entropy(input, target) / math.log(2)\n",
    "\n",
    "# optimizer = lib.optimizer.MyOptimizer(net.parameters(), lr=0.01)\n",
    "t0 = time.time()\n",
    "for epoch in range(50):\n",
    "    print(f\"{time.time() - t0:.3f} sec\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_func(), 1):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss += net.reg_loss_func()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        n_observe = 4 * 195\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f'[{epoch + 1}, {i:5d}] running loss: {running_loss / i:.2f}')\n",
    "    running_loss = 0.0\n",
    "    calc_stats(net, train_whole_data)\n",
    "    calc_stats(net, test_whole_data)\n",
    "    #lr schedule\n",
    "    if True:\n",
    "        observed = np.array(losses[-n_observe:])\n",
    "        av1 = np.average(observed[:len(observed) // 2])\n",
    "        av2 = np.average(observed[len(observed) // 2:])\n",
    "        print(f\"av1={av1:.4f} av2={av2:.4f}\")\n",
    "        slope, pvalue = livenet.stat_utils.get_slope_and_pvalue(losses[-n_observe:])\n",
    "        print(f\"slope={slope:.1e} pvalue={pvalue:.1e} lr={optimizer.param_groups[0]['lr']}\")\n",
    "        if slope >= 0.0:\n",
    "            optimizer.param_groups[0][\"lr\"] /= 1.4\n",
    "            print(f\"reduced lr to {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(lib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "it = iter(net.parameters())\n",
    "conv1 = next(it).data.numpy()\n",
    "next(it)\n",
    "conv2 = next(it).data.numpy()\n",
    "livenet.visual_utils.show_convs(conv1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(lib)\n",
    "stat = lib.stat_utils.AccumStat()\n",
    "for name, param in net.named_parameters():\n",
    "    stat = lib.stat_utils.AccumStat(param.data)\n",
    "    stat.plot(name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
