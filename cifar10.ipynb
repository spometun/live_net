{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO:\n",
    "# reproducible results driven by seed\n",
    "# correlation of features output for two networks. Or even FC training from from to other with L1/L2 norm?\n",
    "# start develop liveconv\n",
    "# don't forger penalise for operation, not (just) weight. Add stats on operations\n",
    "# why features' weights small but not zero? should I try depth-separable conv?\n",
    "# manual livenet for cifar10, and then think on auto?\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torchsummary\n",
    "import numpy as np\n",
    "import gc\n",
    "from livenet.utils import set_seed\n",
    "import onnx\n",
    "import livenet\n",
    "device = \"cuda\"\n",
    "#device = \"cpu\"\n",
    "#torch.set_default_device(device)\n",
    "from ai_libs.simple_log import LOG\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 256\n",
    "from livenet.datasets import TransformDataset\n",
    "test_x, test_y = livenet.datasets.get_cifar10_test()\n",
    "test = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_aug = TransformDataset(test, livenet.datasets.cifar10_test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_aug, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "train_x, train_y = livenet.datasets.get_cifar10_train()\n",
    "train = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_aug = TransformDataset(train, livenet.datasets.cifar10_train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_aug, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=16, pin_memory=True, prefetch_factor=2)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "train_aug = TransformDataset(train, livenet.datasets.cifar10_train_transform)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img = train_aug[4][0].numpy()\n",
    "img = img.transpose(1, 2, 0)\n",
    "from matplotlib import pyplot as plt\n",
    "#img = livenet.datasets._elastic_transform(img, (-4, 0))\n",
    "img = (img * 128 + 127).astype(np.uint8)\n",
    "\n",
    "#plt.imsave(\"/home/spometun/img.png\", img)\n",
    "plt.imshow( img )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T21:27:54.154799Z",
     "start_time": "2025-05-23T21:27:53.480428Z"
    }
   },
   "source": [
    "from cifar_arch import EffNet, ResNet9, ResNet9Small\n",
    "\n",
    "set_seed(2)\n",
    "\n",
    "# network = EffNet(device)\n",
    "network = ResNet9Small(3, 10, device)\n",
    "torchsummary.summary(network, (3, 32, 32), device=device)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "             ReLU6-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 32, 32, 32]           4,640\n",
      "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
      "             ReLU6-6           [-1, 32, 32, 32]               0\n",
      "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
      "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
      "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
      "            ReLU6-10           [-1, 32, 16, 16]               0\n",
      "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
      "            ReLU6-13           [-1, 32, 16, 16]               0\n",
      "           Conv2d-14           [-1, 32, 16, 16]              32\n",
      "           Conv2d-15           [-1, 64, 16, 16]          18,496\n",
      "      BatchNorm2d-16           [-1, 64, 16, 16]             128\n",
      "            ReLU6-17           [-1, 64, 16, 16]               0\n",
      "        MaxPool2d-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 8, 8]          73,856\n",
      "      BatchNorm2d-20            [-1, 128, 8, 8]             256\n",
      "            ReLU6-21            [-1, 128, 8, 8]               0\n",
      "        MaxPool2d-22            [-1, 128, 4, 4]               0\n",
      "           Conv2d-23            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-24            [-1, 128, 4, 4]             256\n",
      "            ReLU6-25            [-1, 128, 4, 4]               0\n",
      "           Conv2d-26            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-27            [-1, 128, 4, 4]             256\n",
      "            ReLU6-28            [-1, 128, 4, 4]               0\n",
      "           Conv2d-29            [-1, 128, 4, 4]             128\n",
      "        MaxPool2d-30            [-1, 128, 1, 1]               0\n",
      "           Conv2d-31             [-1, 10, 1, 1]           1,290\n",
      "          Flatten-32                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 413,674\n",
      "Trainable params: 413,674\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.34\n",
      "Params size (MB): 1.58\n",
      "Estimated Total Size (MB): 3.93\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:30:14.314042Z",
     "start_time": "2025-05-23T21:30:13.990146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dummy_input = torch.zeros(size=(1, 3, 32, 32), device=device)\n",
    "torch.onnx.export(network, dummy_input, \"/home/spometun/temp/resnet9small.onnx\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "ax_in = (0, 2, 3)\n",
    "ax_out = (1, 2, 3)\n",
    "ax = ax_out\n",
    "vals = list(network.parameters())[32].data.cpu().numpy()\n",
    "vals_ref = list(network.parameters())[32].data\n",
    "out_max = np.max(vals, axis=ax)\n",
    "out_min = np.min(vals, axis=ax)\n",
    "sign = -out_min > out_max\n",
    "out = np.max(np.abs(vals), axis=ax)\n",
    "#out *= 1 - sign * 2\n",
    "LOG(f\"{np.min(vals):.2f} {np.max(vals):.2f}\")\n",
    "print(np.sum(np.abs(vals) < 0.01), np.sum(np.abs(vals) > 0.0))\n",
    "#vals_ref[np.abs(vals) < 0.05] = 0.0\n",
    "out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "criterion = livenet.nets.criterion_classification_n\n",
    "optimizer = livenet.nets.create_optimizer(network)\n",
    "trainer = livenet.net_trainer.NetTrainer(network, train_loader, criterion, optimizer)\n",
    "trainer.adaptive_lr = True\n",
    "optimizer.learning_rate = 0.0025\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "network.train()\n",
    "trainer.optimizer.learning_rate = 0.01 / 2 / 2 / 2 / 2 / 2 /2/2\n",
    "network._alpha = 10 * 1e-6\n",
    "trainer.step(1000)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def _infer_epoch(network, loader):\n",
    "    preds = []\n",
    "    ys = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in iter(loader):\n",
    "            x = x.to(network.device)\n",
    "            pred = network(x)\n",
    "            pred = pred.cpu()\n",
    "            preds.append(pred)\n",
    "            ys.append(y.cpu())\n",
    "    pred = torch.concatenate(preds)\n",
    "    y = torch.concatenate(ys)\n",
    "    return pred, y\n",
    "\n",
    "\n",
    "network.train()\n",
    "#train_pred, train_labels = _infer_epoch(network, train_loader)\n",
    "test_pred, test_labels = _infer_epoch(network, test_loader)\n",
    "\n",
    "network.eval()\n",
    "test_pred, test_labels = _infer_epoch(network, test_loader)\n",
    "train_pred, train_labels = _infer_epoch(network, train_loader)\n",
    "\n",
    "def calc_accuracy(predictions, labels):\n",
    "    _, predicted = torch.max(predictions.data, 1)\n",
    "    labels = labels.numpy()\n",
    "    labels = np.squeeze(labels, 1)\n",
    "    predicted = predicted.numpy()\n",
    "    correct = np.sum(predicted == labels)\n",
    "    total = len(labels)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# test_y_tensor = torch.tensor(test_y, device=\"cpu\")\n",
    "# train_y_tensor = torch.tensor(train_y, device=\"cpu\")\n",
    "test_loss = trainer.criterion(test_pred, test_labels).item()\n",
    "train_loss = trainer.criterion(train_pred, train_labels).item()\n",
    "LOG(f\"loss: train: {train_loss:.3f} test: {test_loss:.3f}\")\n",
    "\n",
    "test_accuracy = calc_accuracy(test_pred, test_labels)\n",
    "train_accuracy = calc_accuracy(train_pred, train_labels)\n",
    "LOG(f\"accuracy, train: {100 * train_accuracy:.1f}% test: {100 * test_accuracy:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "network.to(\"cpu\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param = list(network.named_parameters())\n",
    "param0 = list(network.parameters())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "w = param[12][1].detach().cpu().numpy()\n",
    "m = np.max(np.abs(w), axis=(1, 2, 3))\n",
    "f\"{int(100 * np.sum(m > 0.01) / len(m))}%\"\n",
    "np.sum(m > 0.01)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outs = dict()\n",
    "for i in range(11):\n",
    "    ind = i * 4\n",
    "    if ind >= 36:\n",
    "        ind = 32 + (i - 8)\n",
    "    w = param[ind][1].detach().cpu().numpy()\n",
    "    w_name = param[ind][0]\n",
    "    m_out = np.max(np.abs(w), axis=(1, 2, 3))\n",
    "    outs[ind] = m_out > 0.01\n",
    "    m_in = np.max(np.abs(w), axis=(0, 2, 3))\n",
    "    n_out = np.sum(m_out > 0.01)\n",
    "    n_in = np.sum(m_in > 0.01)\n",
    "    s = f\"{w_name} {ind}, {w.shape} {int(100 * np.sum(m_out > 0.01) / len(m_out))}%, {n_in}->{n_out}\"\n",
    "    print(s)\n",
    "for ind in [12, 28]:\n",
    "    ind2 = 32 if ind == 12 else 33\n",
    "    n_inter = np.sum(outs[ind] * outs[ind2])\n",
    "    print(f\"{ind} inter={n_inter}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.set_printoptions(1)\n",
    "w = param[12][1].detach().cpu().numpy()\n",
    "wa = np.abs(w)\n",
    "wa_max = np.max(wa, axis=(1, 2, 3))\n",
    "i = np.argsort(wa_max)[::-1]\n",
    "f = w[i[0]]\n",
    "fa = np.abs(f)\n",
    "fa_max = np.max(fa, axis=(1, 2))\n",
    "#print(f\"{wa_max[i]}\")\n",
    "# print(wa[i].shape)\n",
    "i1 = np.argsort(fa_max)[::-1]\n",
    "f1b = fa > 0.1\n",
    "np.sum(f1b)\n",
    "f[i1[2]]\n",
    "# fa[69]\n",
    "#f[i1]"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
