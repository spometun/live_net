{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-30T21:13:51.968990Z",
     "start_time": "2025-03-30T21:13:48.126572Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from livenet.backend.core import Context\n",
    "import random\n",
    "import importlib\n",
    "import onnx\n",
    "import livenet\n",
    "device = \"cuda\"\n",
    "# device = \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "from ai_libs.simple_log import LOG\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T00:59:54.838103Z",
     "start_time": "2025-03-27T00:59:34.973585Z"
    }
   },
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor()\n",
    "#         ,transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#      ])\n",
    "# train=torchvision.datasets.CIFAR10(\"/home/spometun/datasets/research\", train=True,\n",
    "#                                   download=True, transform=transform)\n",
    "# test=torchvision.datasets.CIFAR10(\"/home/spometun/datasets/research\", train=False,\n",
    "#                                   download=True, transform=transform)\n",
    "#\n",
    "#\n",
    "# def get_whole_data(dataset):\n",
    "#     loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
    "#     data = next(iter(loader))\n",
    "#     data[0] = data[0].to(device)\n",
    "#     data[1] = data[1].to(device)\n",
    "#     return data\n",
    "#\n",
    "# test_whole_data = get_whole_data(test)\n",
    "# train_whole_data = get_whole_data(train)\n",
    "\n",
    "# test_x, test_y = test_whole_data\n",
    "# train_x, train_y = train_whole_data\n",
    "\n",
    "test_x, test_y = livenet.datasets.get_cifar10_test()\n",
    "train_x, train_y = livenet.datasets.get_cifar10_train(augment=True)\n",
    "\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T01:56:12.337612Z",
     "start_time": "2025-03-27T01:56:12.297328Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(0)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.context = Context(self)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.av_pool = nn.AvgPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3)\n",
    "        # self.conv2 = nn.Conv2d(8, 8, 3,padding=\"same\")\n",
    "\n",
    "        self.conv2b = nn.Conv2d(8, 16, 3)\n",
    "        self.conv3b = nn.Conv2d(16, 16, 3, groups=16)\n",
    "        self.conv4b = nn.Conv2d(16, 32, 1)\n",
    "        self.conv5b = nn.Conv2d(32, 32, 3, groups=32)\n",
    "        self.conv6b = nn.Conv2d(32, 64, 1)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc1b = nn.Linear(64, 10)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc = nn.Linear(8*7*7,10)\n",
    "        self._alpha = 0.0001\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # x = self.max_pool(x)\n",
    "        # x = self.av_pool(x)\n",
    "\n",
    "        # x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.bn2(self.conv2b(x)))\n",
    "        x = self.av_pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3b(x)))\n",
    "        x = self.av_pool(x)\n",
    "        x = F.relu(self.bn4(self.conv4b(x)))\n",
    "        x = F.relu(self.bn5(self.conv5b(x)))\n",
    "        x = self.av_pool(x)\n",
    "        x = F.relu(self.bn6(self.conv6b(x)))\n",
    "        x = self.av_pool(x)\n",
    "        # x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        x=self.fc1b(x)\n",
    "        return x\n",
    "\n",
    "    def internal_loss(self):\n",
    "        loss = torch.tensor(0.)\n",
    "        for param in self.parameters():\n",
    "            if len(param.data.shape) > 1:\n",
    "                loss += self._alpha * torch.sum(torch.abs(param)) / param.data.numel()\n",
    "        return loss\n",
    "\n",
    "\n",
    "network = Net()\n",
    "torchsummary.summary(network, (3, 32, 32), device=device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 30, 30]             224\n",
      "       BatchNorm2d-2            [-1, 8, 30, 30]              16\n",
      "            Conv2d-3           [-1, 16, 28, 28]           1,168\n",
      "       BatchNorm2d-4           [-1, 16, 28, 28]              32\n",
      "         AvgPool2d-5           [-1, 16, 14, 14]               0\n",
      "            Conv2d-6           [-1, 16, 12, 12]             160\n",
      "       BatchNorm2d-7           [-1, 16, 12, 12]              32\n",
      "         AvgPool2d-8             [-1, 16, 6, 6]               0\n",
      "            Conv2d-9             [-1, 32, 6, 6]             544\n",
      "      BatchNorm2d-10             [-1, 32, 6, 6]              64\n",
      "           Conv2d-11             [-1, 32, 4, 4]             320\n",
      "      BatchNorm2d-12             [-1, 32, 4, 4]              64\n",
      "        AvgPool2d-13             [-1, 32, 2, 2]               0\n",
      "           Conv2d-14             [-1, 64, 2, 2]           2,112\n",
      "      BatchNorm2d-15             [-1, 64, 2, 2]             128\n",
      "        AvgPool2d-16             [-1, 64, 1, 1]               0\n",
      "           Linear-17                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 5,514\n",
      "Trainable params: 5,514\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.40\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T01:56:27.132061Z",
     "start_time": "2025-03-27T01:56:27.110619Z"
    }
   },
   "source": [
    "batch_size = 256\n",
    "batch_iterator = livenet.gen_utils.batch_iterator(train_x, train_y, batch_size)\n",
    "criterion = livenet.nets.criterion_classification_n\n",
    "optimizer = livenet.nets.create_optimizer(network)\n",
    "trainer = livenet.net_trainer.NetTrainer(network, batch_iterator, criterion, optimizer, epoch_size=len(train_x) // batch_size)\n",
    "trainer.adaptive_lr = True\n",
    "optimizer.learning_rate = 0.01\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T02:09:27.085945Z",
     "start_time": "2025-03-27T02:06:49.822355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#network.learning_rate = 0.0005\n",
    "# optimizer.learning_rate /= 2\n",
    "trainer.step(20000)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iˈ0.000 20279 1.004+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ3.181 20669 1.002+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ6.041 21059 1.001+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ9.265 21449 0.997+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ12.483 21839 0.999+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ15.622 22229 0.995+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ18.761 22619 0.996+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ21.972 23009 0.992+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ25.150 23399 0.993+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ28.110 23789 0.991+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ30.935 24179 0.994+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ33.783 24569 0.987+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ36.634 24959 0.991+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ39.414 25349 0.989+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ42.301 25739 0.985+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ45.076 26129 0.986+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ47.929 26519 0.986+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ50.553 26909 0.984+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ53.735 27299 0.984+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ56.924 27689 0.982+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ59.631 28079 0.981+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ62.522 28469 0.982+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ65.761 28859 0.980+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ68.447 29249 0.979+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ71.663 29639 0.979+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ74.809 30029 0.978+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ77.970 30419 0.977+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ81.055 30809 0.976+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ84.192 31199 0.975+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ87.386 31589 0.974+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ90.654 31979 0.974+0.001reg params=30 lr=0.01000 <string>:34\n",
      "Iˈ93.855 32369 0.976+0.001reg params=30 lr=0.00500 <string>:34\n",
      "Iˈ97.051 32759 0.940+0.001reg params=30 lr=0.00500 <string>:34\n",
      "Iˈ100.245 33149 0.939+0.001reg params=30 lr=0.00500 <string>:34\n",
      "Iˈ103.437 33539 0.938+0.001reg params=30 lr=0.00500 <string>:34\n",
      "Iˈ106.629 33929 0.938+0.001reg params=30 lr=0.00500 <string>:34\n",
      "Iˈ109.825 34319 0.939+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ113.037 34709 0.919+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ116.235 35099 0.920+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ119.447 35489 0.919+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ122.658 35879 0.918+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ125.874 36269 0.918+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ129.062 36659 0.918+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ132.255 37049 0.919+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ135.368 37439 0.917+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ138.474 37829 0.916+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ141.587 38219 0.917+0.001reg params=30 lr=0.00250 <string>:34\n",
      "Iˈ144.704 38609 0.917+0.001reg params=30 lr=0.00125 <string>:34\n",
      "Iˈ147.818 38999 0.908+0.001reg params=30 lr=0.00125 <string>:34\n",
      "Iˈ150.933 39389 0.906+0.001reg params=30 lr=0.00125 <string>:34\n",
      "Iˈ153.828 39779 0.908+0.001reg params=30 lr=0.00125 <string>:34\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T02:09:54.630598Z",
     "start_time": "2025-03-27T02:09:53.629218Z"
    }
   },
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def calc_accuracy(predictions, labels):\n",
    "    _, predicted = torch.max(predictions.data, 1)\n",
    "    labels = labels.cpu().numpy()\n",
    "    labels = np.squeeze(labels, 1)\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    correct = np.sum(predicted == labels)\n",
    "    total = len(labels)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def _infer_in_chunks(network, data):\n",
    "    chunk_size = 256\n",
    "    n = len(data)\n",
    "    preds = []\n",
    "    for i in range( (n + chunk_size - 1) // chunk_size):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, n)\n",
    "        with torch.no_grad():\n",
    "            pred = network(data[start:end])\n",
    "            preds.append(pred)\n",
    "    result = torch.concatenate(preds, dim=0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_loss(trainer, x, y):\n",
    "    with torch.no_grad():\n",
    "        pred = _infer_in_chunks(trainer.network, x)\n",
    "        loss = trainer.criterion(pred, y)\n",
    "        loss = loss.cpu().item()\n",
    "        return loss\n",
    "\n",
    "train_loss = get_loss(trainer, train_x, train_y)\n",
    "test_loss = get_loss(trainer, test_x, test_y)\n",
    "LOG(f\"loss: train: {train_loss:.3f} test: {test_loss:.3f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_outputs = _infer_in_chunks(network, train_x)\n",
    "    test_outputs = _infer_in_chunks(network, test_x)\n",
    "\n",
    "train_accuracy = calc_accuracy(train_outputs, train_y)\n",
    "test_accuracy = calc_accuracy(test_outputs, test_y)\n",
    "LOG(f\"accuracy, train: {100 * train_accuracy:.1f}% test: {100 * test_accuracy:.1f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iˈ0.000 loss: train: 0.905 test: 1.079 \n",
      "Iˈ0.425 accuracy, train: 77.9% test: 73.9% \n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T15:27:53.294358Z",
     "start_time": "2025-03-23T15:27:53.245933Z"
    }
   },
   "source": [
    "def test_func():\n",
    "    return livenet.gen_utils.batch_iterator(*test_whole_data, batch_size=256, only_one_epoch=True)\n",
    "\n",
    "def train_func():\n",
    "    return livenet.gen_utils.batch_iterator(*train_whole_data, batch_size=256, only_one_epoch=True)\n",
    "import time\n",
    "\n",
    "net._alpha = 0.0001\n",
    "\n",
    "def criterion(input, target):\n",
    "    return nn.functional.cross_entropy(input, target) / math.log(2)\n",
    "\n",
    "# optimizer = lib.optimizer.MyOptimizer(net.parameters(), lr=0.01)\n",
    "t0 = time.time()\n",
    "for epoch in range(50):\n",
    "    print(f\"{time.time() - t0:.3f} sec\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_func(), 1):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss += net.reg_loss_func()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        n_observe = 4 * 195\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f'[{epoch + 1}, {i:5d}] running loss: {running_loss / i:.2f}')\n",
    "    running_loss = 0.0\n",
    "    calc_stats(net, train_whole_data)\n",
    "    calc_stats(net, test_whole_data)\n",
    "    #lr schedule\n",
    "    if True:\n",
    "        observed = np.array(losses[-n_observe:])\n",
    "        av1 = np.average(observed[:len(observed) // 2])\n",
    "        av2 = np.average(observed[len(observed) // 2:])\n",
    "        print(f\"av1={av1:.4f} av2={av2:.4f}\")\n",
    "        slope, pvalue = livenet.stat_utils.get_slope_and_pvalue(losses[-n_observe:])\n",
    "        print(f\"slope={slope:.1e} pvalue={pvalue:.1e} lr={optimizer.param_groups[0]['lr']}\")\n",
    "        if slope >= 0.0:\n",
    "            optimizer.param_groups[0][\"lr\"] /= 1.4\n",
    "            print(f\"reduced lr to {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "print('Finished Training')\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m livenet.gen_utils.batch_iterator(*train_whole_data, batch_size=\u001B[32m256\u001B[39m, only_one_epoch=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtime\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[43mnet\u001B[49m._alpha = \u001B[32m0.0001\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcriterion\u001B[39m(\u001B[38;5;28minput\u001B[39m, target):\n\u001B[32m     11\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m nn.functional.cross_entropy(\u001B[38;5;28minput\u001B[39m, target) / math.log(\u001B[32m2\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'net' is not defined"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
