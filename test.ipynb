{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-12T22:47:48.230682Z",
     "start_time": "2024-07-12T22:47:48.194106Z"
    }
   },
   "source": [
    "import copy\n",
    "# TODO: fix die-adaptive lr, show usefull stat instead of logs\n",
    "# \"batch norm analog\" - i.e. collector neuron - when things will go slow with reasonable dataset - likely cifar10, then see if speed improvement\n",
    "# TODO: \n",
    "#  1. investigate living dangle Neurons\n",
    "#  2. investigate gradient size (bigger than lr? limit to lr)\n",
    "#  3. Collect stats on gradients, neuron values, over training step\n",
    "#  4. Fix scrolling\n",
    "#  5. Use Cuda\n",
    "#  6a. Relu -> Relu6\n",
    "#  6b. Improve death, introduce train mode/ death mode (kill any which reach zero)\n",
    "# After that can think on Cifar10 convnet death\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import importlib\n",
    "from ai_libs.basic.deep_reload import deep_reload\n",
    "import livenet\n",
    "deep_reload(livenet)\n",
    "import ai_libs.simple_log as simple_log\n",
    "from ai_libs.simple_log import LOG\n",
    "simple_log.level = simple_log.LogLevel.INFO\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ion()\n",
    "%matplotlib TkAgg\n",
    "livenet.core.utils.set_seed()\n",
    "var = 42\n",
    "print(torch.__version__)\n",
    "np.set_printoptions(precision=3)\n",
    "%precision 4\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'%.4f'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T00:03:28.139092Z",
     "start_time": "2024-07-09T00:03:25.041740Z"
    }
   },
   "source": [
    "downscale = (2, 2)\n",
    "test = livenet.datasets.get_mnist_test()\n",
    "train = livenet.datasets.get_mnist_train()\n",
    "test_x, test_y = livenet.datasets.to_plain(*test, downscale=downscale, to_gray=True, to_odd=True)\n",
    "train_x, train_y = livenet.datasets.to_plain(*train, downscale=downscale, to_gray=True, to_odd=True)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T20:30:28.545074Z",
     "start_time": "2024-07-10T20:30:28.540602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "train_x, train_y = livenet.datasets.get_odd_2()\n",
    "test_x, test_y = train_x, train_y\n"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T15:43:01.393227Z",
     "start_time": "2024-07-09T15:43:01.383854Z"
    }
   },
   "source": [
    "network = livenet.nets.create_perceptron(train_x.shape[1], 1, 2)\n",
    "batch_iterator = livenet.gen_utils.batch_iterator(train_x, train_y, batch_size=3)\n",
    "criterion = livenet.nets.criterion_classification_n\n",
    "optimizer = livenet.nets.create_optimizer(network)\n",
    "optimizer.learning_rate = 0.01\n",
    "trainer = livenet.net_trainer.NetTrainer(network, batch_iterator, criterion, optimizer, epoch_size=1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iˈ0.000 S0 became useless livenet/core/death.py:68\n",
      "Iˈ0.001 D0 became dangle livenet/core/death.py:59\n",
      "Iˈ0.003 D1 became dangle livenet/core/death.py:59\n",
      "Iˈ0.003 N0 became useless livenet/core/death.py:68\n",
      "Iˈ0.004 N0 became dangle livenet/core/death.py:59\n",
      "LiveNet\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:43:02.403192Z",
     "start_time": "2024-07-09T15:43:02.396533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    network.inputs[0].axons[0].destination.b[...] = 200\n",
    "    network.inputs[0].axons[0].destination.axons[0].k[...] = -10.2\n",
    "    network.inputs[0].axons[0].destination.axons[1].k[...] = 10.1\n",
    "\n",
    "train_x[...] = 0.0\n",
    "\n",
    "\n",
    "#network.inputs[0].axons[0].destination.axons[1].k[...]\n",
    "#network.inputs[0].axons[0].destination.b[...]\n",
    "#network.inputs[0].axons[0].destination.axons[0].k[]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T15:43:04.364691Z",
     "start_time": "2024-07-09T15:43:04.068790Z"
    }
   },
   "source": [
    "# simple_log.level = simple_log.LogLevel.DEBUG\n",
    "network.context.regularization_l1 = 0.001\n",
    "optimizer.learning_rate = 0.01\n",
    "trainer.step(3500)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iˈ0.000 3904.895 = 3904.895+0.000reg params=6 livenet/net_trainer.py:93\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m network\u001B[38;5;241m.\u001B[39mcontext\u001B[38;5;241m.\u001B[39mregularization_l1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m\n\u001B[1;32m      3\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.01\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3500\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/home_project/livenet/net_trainer.py:31\u001B[0m, in \u001B[0;36mNetTrainer.step\u001B[0;34m(self, n_steps)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_steps):\n\u001B[0;32m---> 31\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/home_project/livenet/net_trainer.py:44\u001B[0m, in \u001B[0;36mNetTrainer._step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     41\u001B[0m all_loss \u001B[38;5;241m=\u001B[39m loss \u001B[38;5;241m+\u001B[39m loss_network\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 44\u001B[0m \u001B[43mall_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madaptive_lr:\n",
      "File \u001B[0;32m~/miniconda3/envs/home/lib/python3.12/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/home/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/home/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-11T03:03:46.308266Z",
     "start_time": "2024-07-11T03:03:46.303395Z"
    }
   },
   "source": [
    "pred = network(test_x)\n",
    "pred_bin = np.argmax(pred.detach().numpy(), axis=1, keepdims=True)\n",
    "diff = test_y.numpy() - pred_bin\n",
    "f\"accuracy {len(diff[diff == 0]) / len(diff)}\"\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy 0.6666666666666666'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-11T17:15:17.249959Z",
     "start_time": "2024-07-11T17:15:17.037908Z"
    }
   },
   "source": [
    "%matplotlib\n",
    "plt.ion()\n",
    "\n",
    "def param_amount(param):\n",
    "    val = len(param[\"params\"])\n",
    "    return val\n",
    "\n",
    "def param_max(param):\n",
    "    val = np.max(np.abs(param[\"params\"]))\n",
    "    return val\n",
    "\n",
    "def param_picker1(param):\n",
    "    try:\n",
    "        val0 = param[\"params\"][\"N0->D0\"].item()\n",
    "        # val0 = param[\"loss\"]\n",
    "    except KeyError:\n",
    "        val0 = 0.\n",
    "    return val0\n",
    "\n",
    "def get_param_values(history, picker):\n",
    "    values = []\n",
    "    for entry in history:\n",
    "        values.append(picker(entry))\n",
    "    return values\n",
    "\n",
    "#plt.figure(figsize=(16, 9))\n",
    "plt.grid()\n",
    "values = get_param_values(trainer.history, param_picker1)\n",
    "plt.plot(values)\n",
    "# accum = core.stat_utils.AccumStat()\n",
    "# accum.add_value(trainer.network.parameters())\n",
    "# accum.plot()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: tkagg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c3f3061e120>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-11T03:07:08.061032Z",
     "start_time": "2024-07-11T03:07:08.053394Z"
    }
   },
   "source": [
    "print(network.context.topology_stat.get_stat())\n",
    "for name, p in sorted(network.named_parameters(),key=lambda x: len(x[0]) + ord(x[0][0]) / 1000. + ord(x[0][-1]) / 10000):\n",
    "    print(name, f\"{p.item():.2f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dangle': {'RegularNeuron': 1, 'DestinationNeuron': 0}, 'useless': {'RegularNeuron': 0, 'DataNeuron': 1}}\n",
      "D0 -30.38\n",
      "D1 0.00\n",
      "N0 20.00\n",
      "N0->D0 1.55\n",
      "N0->D1 0.00\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T21:33:24.606307Z",
     "start_time": "2024-07-06T21:33:24.600090Z"
    }
   },
   "cell_type": "code",
   "source": "[x.name for x in network.context.topology_stat.dangle]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N3', 'N11']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T22:47:54.460948Z",
     "start_time": "2024-07-12T22:47:54.449742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_symmetric_dangle_net():\n",
    "    from livenet.core.livenet import DestinationNeuron, RegularNeuron, DataNeuron\n",
    "    net = livenet.core.livenet.LiveNet()\n",
    "    net.outputs += [DestinationNeuron(net.context, activation=None), DestinationNeuron(net.context, activation=None)]\n",
    "    net.inputs += [DataNeuron(net.context)]\n",
    "    neuron = RegularNeuron(net.context, activation=torch.nn.ReLU())\n",
    "    neuron.connect_to(net.outputs[0])\n",
    "    neuron.connect_to(net.outputs[1])\n",
    "    # net.inputs[0].connect_to(neuron)\n",
    "    net.root.visit_member(\"init_weight\")\n",
    "    with torch.no_grad():\n",
    "        net.outputs[0].b[...] = -30.0\n",
    "        #net.outputs[0].b.requires_grad = False\n",
    "        net.outputs[1].b[...] = -0\n",
    "        net.outputs[1].b.requires_grad = False\n",
    "        neuron.b[...] = 20.0\n",
    "        neuron.b.requires_grad = False\n",
    "        neuron.axons[0].k[...] = 2.0\n",
    "        # neuron.axons[0].k.requires_grad = False\n",
    "        neuron.axons[1].k[...] = -0\n",
    "        neuron.axons[1].k.requires_grad = False\n",
    "    return net\n",
    "\n",
    "train_x, train_y = livenet.datasets.get_odd_2()  \n",
    "test_x, test_y = train_x, train_y\n",
    "# train_x is not actually used\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T22:47:55.017833Z",
     "start_time": "2024-07-12T22:47:55.014752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "network = build_symmetric_dangle_net()\n",
    "# train_x is not actually used as networks input doesn't connected to anything\n",
    "batch_iterator = livenet.gen_utils.batch_iterator(train_x, train_y, batch_size=3)\n",
    "criterion = livenet.nets.criterion_classification_n\n",
    "optimizer = livenet.nets.create_optimizer(network)\n",
    "trainer = livenet.net_trainer.NetTrainer(network, batch_iterator, criterion, optimizer, epoch_size=1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iˈ0.000 D0 became dangle livenet/core/observability.py:14\n",
      "Iˈ0.000 D1 became dangle livenet/core/observability.py:14\n",
      "Iˈ0.000 S0 became useless livenet/core/observability.py:23\n",
      "Iˈ0.001 N0 became useless livenet/core/observability.py:23\n",
      "Iˈ0.001 N0 became dangle livenet/core/observability.py:14\n",
      "LiveNet\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T22:47:56.341868Z",
     "start_time": "2024-07-12T22:47:56.330461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "network.context.regularization_l1 = 0.001\n",
    "optimizer.learning_rate = 0.01\n",
    "trainer.step(1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iˈ0.000 4.811 = 4.809+0.002reg params=5 livenet/net_trainer.py:93\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T22:47:58.448899Z",
     "start_time": "2024-07-12T22:47:58.433567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "life_stat = network.context.life_stat\n",
    "pd.DataFrame(life_stat)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         type      value    name         class_name  tick\n",
       "0      output  20.000000      N0      RegularNeuron     0\n",
       "1      output  40.000000  N0->D0            Synapse     0\n",
       "2      output  10.000000      D0  DestinationNeuron     0\n",
       "3      output   0.000000  N0->D1            Synapse     0\n",
       "4      output   0.000000      D1  DestinationNeuron     0\n",
       "5   parameter -30.000000      D0   AdamForParameter     0\n",
       "6    gradient   0.480833      D0   AdamForParameter     0\n",
       "7       delta  -0.010000      D0   AdamForParameter     0\n",
       "8   parameter   2.000000  N0->D0   AdamForParameter     0\n",
       "9    gradient   9.617725  N0->D0   AdamForParameter     0\n",
       "10      delta  -0.010000  N0->D0   AdamForParameter     0\n",
       "11  parameter  20.000000      N0   AdamForParameter     0\n",
       "12  parameter   0.000000      D1   AdamForParameter     0\n",
       "13  parameter   0.000000  N0->D1   AdamForParameter     0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>name</th>\n",
       "      <th>class_name</th>\n",
       "      <th>tick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>output</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>N0</td>\n",
       "      <td>RegularNeuron</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>output</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>N0-&gt;D0</td>\n",
       "      <td>Synapse</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>output</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>D0</td>\n",
       "      <td>DestinationNeuron</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>output</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>N0-&gt;D1</td>\n",
       "      <td>Synapse</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>output</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>D1</td>\n",
       "      <td>DestinationNeuron</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>parameter</td>\n",
       "      <td>-30.000000</td>\n",
       "      <td>D0</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gradient</td>\n",
       "      <td>0.480833</td>\n",
       "      <td>D0</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>delta</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>D0</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>parameter</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>N0-&gt;D0</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gradient</td>\n",
       "      <td>9.617725</td>\n",
       "      <td>N0-&gt;D0</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>delta</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>N0-&gt;D0</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>parameter</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>N0</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>parameter</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>D1</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>parameter</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>N0-&gt;D1</td>\n",
       "      <td>AdamForParameter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
