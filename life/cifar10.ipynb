{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'gen_utils' from '/home/spometun/projects/solink/scripts/python/home_project/life/gen_utils.py'>"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "import gen_utils\n",
    "importlib.reload(gen_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()\n",
    "        ,transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "     ])\n",
    "train=torchvision.datasets.CIFAR10(\"/home/spometun/datasets/research\", train=True,\n",
    "                                  download=True, transform=transform)\n",
    "test=torchvision.datasets.CIFAR10(\"/home/spometun/datasets/research\", train=False,\n",
    "                                  download=True, transform=transform)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(2)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# torchsummary.summary(net, (3, 32, 32), device=torch.device(\"cpu\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "def get_whole_data(dataset):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
    "    data = next(iter(loader))\n",
    "    return data\n",
    "\n",
    "train_whole_data = get_whole_data(train)\n",
    "test_whole_data = get_whole_data(test)\n",
    "\n",
    "def calc_stats(network, data):\n",
    "    with torch.no_grad():\n",
    "        outputs = network(data[0])\n",
    "    labels = data[1]\n",
    "\n",
    "    total = len(labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    _loss = criterion(outputs, labels)\n",
    "    print(f\"Loss {_loss: .2f} Accuracy {100 * correct / total:.1f}%\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] running loss: 1.30\n",
      "Loss  1.24 Accuracy 69.4%\n",
      "Loss  1.51 Accuracy 64.4%\n",
      "[1,  4000] running loss: 1.24\n",
      "Loss  1.21 Accuracy 70.3%\n",
      "Loss  1.49 Accuracy 64.6%\n",
      "[1,  6000] running loss: 1.27\n",
      "Loss  1.22 Accuracy 70.1%\n",
      "Loss  1.51 Accuracy 64.0%\n",
      "[1,  8000] running loss: 1.26\n",
      "Loss  1.17 Accuracy 71.4%\n",
      "Loss  1.47 Accuracy 65.0%\n",
      "[1, 10000] running loss: 1.25\n",
      "Loss  1.17 Accuracy 71.3%\n",
      "Loss  1.48 Accuracy 64.5%\n",
      "[1, 12000] running loss: 1.24\n",
      "Loss  1.14 Accuracy 72.2%\n",
      "Loss  1.46 Accuracy 65.3%\n",
      "[2,  2000] running loss: 1.16\n",
      "Loss  1.14 Accuracy 72.1%\n",
      "Loss  1.49 Accuracy 65.5%\n",
      "[2,  4000] running loss: 1.19\n",
      "Loss  1.15 Accuracy 71.9%\n",
      "Loss  1.49 Accuracy 64.8%\n",
      "[2,  6000] running loss: 1.18\n",
      "Loss  1.14 Accuracy 72.0%\n",
      "Loss  1.48 Accuracy 64.8%\n",
      "[2,  8000] running loss: 1.15\n",
      "Loss  1.11 Accuracy 72.8%\n",
      "Loss  1.47 Accuracy 65.0%\n",
      "[2, 10000] running loss: 1.16\n",
      "Loss  1.11 Accuracy 72.4%\n",
      "Loss  1.47 Accuracy 65.1%\n",
      "[2, 12000] running loss: 1.18\n",
      "Loss  1.10 Accuracy 73.0%\n",
      "Loss  1.47 Accuracy 65.4%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=4, shuffle=True)\n",
    "\n",
    "def criterion(input, target):\n",
    "    return nn.functional.cross_entropy(input, target) / math.log(2)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.0002, momentum=0.9)\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 0:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i:5d}] running loss: {running_loss / 2000:.2f}')\n",
    "            running_loss = 0.0\n",
    "            calc_stats(net, train_whole_data)\n",
    "            calc_stats(net, test_whole_data)\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
